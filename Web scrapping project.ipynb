{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3b9b5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Header Tag\n",
      "0                      Main Page\n",
      "1           Welcome to Wikipedia\n",
      "2  From today's featured article\n",
      "3               Did you know ...\n",
      "4                    In the news\n",
      "5                    On this day\n",
      "6       Today's featured picture\n",
      "7       Other areas of Wikipedia\n",
      "8    Wikipedia's sister projects\n",
      "9            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to display all the header tags from wikipedia.org and make data frame.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "data = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "ab = BeautifulSoup(data.content, \"html.parser\")\n",
    "header_tags = ab.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "header_tag_texts = []\n",
    "for header_tag in header_tags:\n",
    "    header_tag_texts.append(header_tag.text)\n",
    "    \n",
    "header_tag_df = pd.DataFrame({\"Header Tag\": header_tag_texts})\n",
    "print(header_tag_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3647781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            Name   Term of Office\n",
      "0                     Start Date     Closing date\n",
      "1            Dr. Rajendra Prasad  26 January 1950\n",
      "2   Dr. Sarvepalli Radhakrishnan      13 May 1962\n",
      "3              Dr. Zakir Hussain      13 May 1967\n",
      "4        Varahagiri Venkata Giri       3 May 1969\n",
      "5        Varahagiri Venkata Giri   24 August 1969\n",
      "6           Fakhruddin Ali Ahmed   24 August 1974\n",
      "7           Neelam Sanjiva Reddy     25 July 1977\n",
      "8               Giani Zali Singh     25 July 1982\n",
      "9         Ramaswamy Venkataraman     25 July 1987\n",
      "10          Shankar Dayal Sharma     25 July 1992\n",
      "11      Kocheril Raman Narayanan     25 July 1997\n",
      "12        Dr. A.P.J. Abdul Kalam     25 July 2002\n",
      "13                Pratibha Patil     25 July 2007\n",
      "14              Pranab Mukherjee     25 July 2012\n",
      "15          Shri Ram Nath Kovind     25 July 2017\n",
      "16                Draupadi Murmu     21 July 2022\n"
     ]
    }
   ],
   "source": [
    "#  Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "# from \"https://currentaffairs.adda247.com/list-of-presidents-of-india/\" and make data frame.\n",
    "    \n",
    "    \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "link = \"https://currentaffairs.adda247.com/list-of-presidents-of-india/\"\n",
    "res = requests.get(url)\n",
    "if res.status_code == 200:\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    tab = soup.find('table')\n",
    "    names = []\n",
    "    terms = []\n",
    "    for row in tab.find_all('tr')[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        name = columns[0].text.strip()\n",
    "        term = columns[1].text.strip()\n",
    "        names.append(name)\n",
    "        terms.append(term)\n",
    "    df = pd.DataFrame({'Name': names, 'Term of Office': terms})\n",
    "    print(df)\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60dc033b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "  Rank              Name Team Rating\n",
      "0    1    Australia\\nAUS   27  3,112\n",
      "1    2     Pakistan\\nPAK   27  3,102\n",
      "2    3        India\\nIND   40  4,558\n",
      "3    4      England\\nENG   28  2,942\n",
      "4    5  South Africa\\nSA   23  2,386\n",
      "5    6   New Zealand\\nNZ   31  3,110\n",
      "6    7   Bangladesh\\nBAN   33  3,107\n",
      "7    8     Sri Lanka\\nSL   37  3,448\n",
      "8    9  Afghanistan\\nAFG   21  1,687\n",
      "9   10   West Indies\\nWI   38  2,582\n",
      " Top 10 ODI Batsmen:\n",
      "                                                Rank                   Name  \\\n",
      "0               1\\n                        \\n\\n\\n(0)             Babar Azam   \n",
      "1       2\\n                                \\n\\n\\n(0)           Shubman Gill   \n",
      "2       3\\n                                \\n\\n\\n(0)  Rassie van der Dussen   \n",
      "3  4\\n                                \\n\\n\\n\\n\\n(...           David Warner   \n",
      "4  5\\n                                \\n\\n\\n\\n\\n(...            Imam-ul-Haq   \n",
      "5       6\\n                                \\n\\n\\n(0)           Harry Tector   \n",
      "6  7\\n                                \\n\\n\\n\\n\\n(...        Quinton de Kock   \n",
      "7  8\\n                                \\n\\n\\n\\n\\n(...            Virat Kohli   \n",
      "8  9\\n                                \\n\\n\\n\\n\\n(...           Rohit Sharma   \n",
      "9  10\\n                                \\n\\n\\n\\n\\n...           Fakhar Zaman   \n",
      "\n",
      "  Team Rating  \n",
      "0  PAK    863  \n",
      "1  IND    759  \n",
      "2   SA    745  \n",
      "3  AUS    739  \n",
      "4  PAK    735  \n",
      "5  IRE    726  \n",
      "6   SA    721  \n",
      "7  IND    715  \n",
      "8  IND    707  \n",
      "9  PAK    705  \n",
      " Top 10 ODI Bowlers:\n",
      "                                                Rank              Name Team  \\\n",
      "0               1\\n                        \\n\\n\\n(0)    Josh Hazlewood  AUS   \n",
      "1       2\\n                                \\n\\n\\n(0)    Mitchell Starc  AUS   \n",
      "2  =\\n                                \\n\\n\\n\\n\\n(...       Trent Boult   NZ   \n",
      "3  4\\n                                \\n\\n\\n\\n\\n(...        Adam Zampa  AUS   \n",
      "4       5\\n                                \\n\\n\\n(0)        Matt Henry   NZ   \n",
      "5       6\\n                                \\n\\n\\n(0)  Mujeeb Ur Rahman  AFG   \n",
      "6  7\\n                                \\n\\n\\n\\n\\n(...     Kuldeep Yadav  IND   \n",
      "7  8\\n                                \\n\\n\\n\\n\\n(...       Rashid Khan  AFG   \n",
      "8       9\\n                                \\n\\n\\n(0)    Mohammed Siraj  IND   \n",
      "9      10\\n                                \\n\\n\\n(0)    Shaheen Afridi  PAK   \n",
      "\n",
      "  Rating  \n",
      "0    692  \n",
      "1    666  \n",
      "2    666  \n",
      "3    663  \n",
      "4    658  \n",
      "5    657  \n",
      "6    656  \n",
      "7    655  \n",
      "8    643  \n",
      "9    635  \n"
     ]
    }
   ],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame.\n",
    "# a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "# b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "# c) Top 10 ODI bowlers along with the records of their team andrating.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "odi_teams_url = \"https://www.icc-cricket.com/rankings/mens/team-rankings/odi\"\n",
    "\n",
    "odi_batsmen_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting\"\n",
    "\n",
    "odi_bowlers_url = \"https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling\"\n",
    "\n",
    "def scrape_and_create_dataframe(url, limit=10):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for row in table.find_all('tr')[1:limit + 1]:\n",
    "            columns = row.find_all('td')\n",
    "            rank = columns[0].text.strip()\n",
    "            name = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            data.append([rank, name, team, rating])\n",
    "\n",
    "        df = pd.DataFrame(data, columns=[\"Rank\", \"Name\", \"Team\", \"Rating\"])\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "odi_teams_df = scrape_and_create_dataframe(odi_teams_url)\n",
    "odi_batsmen_df = scrape_and_create_dataframe(odi_batsmen_url)\n",
    "odi_bowlers_df = scrape_and_create_dataframe(odi_bowlers_url)\n",
    "\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "print(odi_teams_df)\n",
    "\n",
    "print(\" Top 10 ODI Batsmen:\")\n",
    "print(odi_batsmen_df)\n",
    "\n",
    "print(\" Top 10 ODI Bowlers:\")\n",
    "print(odi_bowlers_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b5f09e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams in Women's Cricket:\n",
      "  Rank              Name Team Rating\n",
      "0    1    Australia\\nAUS   26  4,290\n",
      "1    2      England\\nENG   31  3,875\n",
      "2    3  South Africa\\nSA   26  3,098\n",
      "3    4        India\\nIND   30  3,039\n",
      "4    5   New Zealand\\nNZ   28  2,688\n",
      "5    6   West Indies\\nWI   29  2,743\n",
      "6    7   Bangladesh\\nBAN   17  1,284\n",
      "7    8     Sri Lanka\\nSL   12    820\n",
      "8    9     Thailand\\nTHA   13    883\n",
      "9   10     Pakistan\\nPAK   27  1,678\n",
      "Top 10 Women's ODI Batting Players:\n",
      "                                            Rank                  Name Team  \\\n",
      "0           1\\n                        \\n\\n\\n(0)  Natalie Sciver-Brunt  ENG   \n",
      "1   2\\n                                \\n\\n\\n(0)           Beth Mooney  AUS   \n",
      "2   3\\n                                \\n\\n\\n(0)   Chamari Athapaththu   SL   \n",
      "3   4\\n                                \\n\\n\\n(0)       Laura Wolvaardt   SA   \n",
      "4   =\\n                                \\n\\n\\n(0)       Smriti Mandhana  IND   \n",
      "5   6\\n                                \\n\\n\\n(0)          Alyssa Healy  AUS   \n",
      "6   7\\n                                \\n\\n\\n(0)      Harmanpreet Kaur  IND   \n",
      "7   8\\n                                \\n\\n\\n(0)          Ellyse Perry  AUS   \n",
      "8   9\\n                                \\n\\n\\n(0)           Meg Lanning  AUS   \n",
      "9  10\\n                                \\n\\n\\n(0)       Stafanie Taylor   WI   \n",
      "\n",
      "  Rating  \n",
      "0    801  \n",
      "1    751  \n",
      "2    743  \n",
      "3    708  \n",
      "4    708  \n",
      "5    702  \n",
      "6    694  \n",
      "7    686  \n",
      "8    682  \n",
      "9    618  \n",
      "Top 10 Women's ODI All-rounders:\n",
      "                                            Rank                  Name Team  \\\n",
      "0           1\\n                        \\n\\n\\n(0)  Natalie Sciver-Brunt  ENG   \n",
      "1   2\\n                                \\n\\n\\n(0)      Ashleigh Gardner  AUS   \n",
      "2   3\\n                                \\n\\n\\n(0)       Hayley Matthews   WI   \n",
      "3   4\\n                                \\n\\n\\n(0)        Marizanne Kapp   SA   \n",
      "4   5\\n                                \\n\\n\\n(0)          Ellyse Perry  AUS   \n",
      "5   6\\n                                \\n\\n\\n(0)           Amelia Kerr   NZ   \n",
      "6   7\\n                                \\n\\n\\n(0)         Deepti Sharma  IND   \n",
      "7   8\\n                                \\n\\n\\n(0)         Jess Jonassen  AUS   \n",
      "8   9\\n                                \\n\\n\\n(0)         Sophie Devine   NZ   \n",
      "9  10\\n                                \\n\\n\\n(0)              Nida Dar  PAK   \n",
      "\n",
      "  Rating  \n",
      "0    398  \n",
      "1    389  \n",
      "2    382  \n",
      "3    362  \n",
      "4    329  \n",
      "5    328  \n",
      "6    312  \n",
      "7    241  \n",
      "8    233  \n",
      "9    217  \n"
     ]
    }
   ],
   "source": [
    "# Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame.\n",
    "# a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "# b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "# c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_and_create_dataframe(url, limit=10):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for row in table.find_all('tr')[1:limit + 1]:\n",
    "            columns = row.find_all('td')\n",
    "            rank = columns[0].text.strip()\n",
    "            name = columns[1].text.strip()\n",
    "            team = columns[2].text.strip()\n",
    "            rating = columns[3].text.strip()\n",
    "            data.append([rank, name, team, rating])\n",
    "\n",
    "        df = pd.DataFrame(data, columns=[\"Rank\", \"Name\", \"Team\", \"Rating\"])\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "        return None\n",
    "odi_teams_url = \"https://www.icc-cricket.com/rankings/womens/team-rankings/odi\"\n",
    "odi_teams_df = scrape_and_create_dataframe(odi_teams_url)\n",
    "if odi_teams_df is not None:\n",
    "    print(\"Top 10 ODI Teams in Women's Cricket:\")\n",
    "    print(odi_teams_df)\n",
    "    \n",
    "odi_batsmen_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting\"\n",
    "odi_batsmen_df = scrape_and_create_dataframe(odi_batsmen_url)\n",
    "if odi_batsmen_df is not None:\n",
    "    print(\"Top 10 Women's ODI Batting Players:\")\n",
    "    print(odi_batsmen_df)\n",
    "    \n",
    "odi_allrounders_url = \"https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder\"\n",
    "odi_allrounders_df = scrape_and_create_dataframe(odi_allrounders_url)\n",
    "if odi_allrounders_df is not None:\n",
    "    print(\"Top 10 Women's ODI All-rounders:\")\n",
    "    print(odi_allrounders_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23b55d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Headline, Time, News Link]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "# make data frame.\n",
    "# i) Headline\n",
    "# ii) Time\n",
    "# iii) News Link\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "def scrape_news_details(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        headlines = []\n",
    "        times = []\n",
    "        news_links = []\n",
    "        articles = soup.find_all('div', class_='Card-title')\n",
    "\n",
    "        for article in articles:\n",
    "            headline = article.text.strip()\n",
    "            headlines.append(headline)\n",
    "            parent_div = article.find_parent('div', class_='Card-info')\n",
    "            time = parent_div.find('time')\n",
    "            if time:\n",
    "                time = time.text.strip()\n",
    "            else:\n",
    "                time = \"N/A\"\n",
    "            times.append(time)\n",
    "            link = parent_div.find('a', class_='Card-image')\n",
    "            if link:\n",
    "                news_link = link['href']\n",
    "            else:\n",
    "                news_link = \"N/A\"\n",
    "            news_links.append(news_link)\n",
    "        data = {\n",
    "            \"Headline\": headlines,\n",
    "            \"Time\": times,\n",
    "            \"News Link\": news_links\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "        return None\n",
    "    \n",
    "news_df = scrape_news_details(url)\n",
    "if news_df is not None:\n",
    "    print(news_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54b54fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Paper Title, Authors, Published Date, Paper URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "# days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "# Scrape below mentioned details and make data frame\n",
    "# i) Paper Title\n",
    "# ii) Authors\n",
    "# iii) Published Date\n",
    "# iv) Paper URL\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        paper_titles = []\n",
    "        authors_list = []\n",
    "        published_dates = []\n",
    "        paper_urls = []\n",
    "        articles = soup.find_all('div', class_='js-most-downloaded__articles__article')\n",
    "\n",
    "        for article in articles:\n",
    "            paper_title = article.find('a', class_='article-title')\n",
    "            if paper_title:\n",
    "                paper_title = paper_title.text.strip()\n",
    "            else:\n",
    "                paper_title = \"N/A\"\n",
    "            paper_titles.append(paper_title)\n",
    "\n",
    "            authors = article.find('div', class_='authors')\n",
    "            if authors:\n",
    "                authors = authors.text.strip()\n",
    "            else:\n",
    "                authors = \"N/A\"\n",
    "            authors_list.append(authors)\n",
    "            \n",
    "            published_date = article.find('div', class_='text-xs')\n",
    "            if published_date:\n",
    "                published_date = published_date.text.strip()\n",
    "            else:\n",
    "                published_date = \"N/A\"\n",
    "            published_dates.append(published_date)\n",
    "\n",
    "            paper_url = article.find('a', class_='article-title')['href']\n",
    "            if paper_url:\n",
    "                paper_url = paper_url.strip()\n",
    "            else:\n",
    "                paper_url = \"N/A\"\n",
    "            paper_urls.append(paper_url)\n",
    "\n",
    "        data = {\n",
    "            \"Paper Title\": paper_titles,\n",
    "            \"Authors\": authors_list,\n",
    "            \"Published Date\": published_dates,\n",
    "            \"Paper URL\": paper_urls\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "        return None\n",
    "    \n",
    "articles_df = scrape_most_downloaded_articles(url)\n",
    "\n",
    "if articles_df is not None:\n",
    "    print(articles_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "729c5c9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Write a python program to scrape mentioned details from dineout.co.inand make data frame\n",
    "# i) Restaurant name\n",
    "# ii) Cuisine\n",
    "# iii) Location\n",
    "# iv) Ratings\n",
    "# v) Image URL\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants\"\n",
    "\n",
    "def scrape_restaurant_details(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        restaurant_names = []\n",
    "        cuisines = []\n",
    "        locations = []\n",
    "        ratings = []\n",
    "        image_urls = []\n",
    "        \n",
    "        restaurants = soup.find_all('div', class_='listing-details-right')\n",
    "\n",
    "        for restaurant in restaurants:\n",
    "         \n",
    "            name = restaurant.find('div', class_='restnt-info-main').a.h2.text.strip()\n",
    "            restaurant_names.append(name)\n",
    "            cuisine = restaurant.find('span', class_='double-line-ellipsis').text.strip()\n",
    "            cuisines.append(cuisine)\n",
    "            location = restaurant.find('span', class_='double-line-ellipsis-grey').text.strip()\n",
    "            locations.append(location)\n",
    "            rating = restaurant.find('span', class_='rating-val').text.strip()\n",
    "            ratings.append(rating)   \n",
    "            image_url = restaurant.find('img', class_='no-img')['data-src']\n",
    "            image_urls.append(image_url)\n",
    "\n",
    "        \n",
    "        data = {\n",
    "            \"Restaurant Name\": restaurant_names,\n",
    "            \"Cuisine\": cuisines,\n",
    "            \"Location\": locations,\n",
    "            \"Ratings\": ratings,\n",
    "            \"Image URL\": image_urls\n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "    else:\n",
    "        print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "        return None\n",
    "restaurant_df = scrape_restaurant_details(url)\n",
    "if restaurant_df is not None:\n",
    "    print(restaurant_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3f22a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60dbef29",
   "metadata": {},
   "source": [
    "# 1. Write a python program which searches all the product under a particular product from www.amazon.in. The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search for guitars.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a17aa46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product to search: guitar\n",
      "Products found:\n"
     ]
    }
   ],
   "source": [
    "# !pip install bs4\n",
    "# !pip install requests\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_amazon(product):\n",
    " \n",
    "\n",
    "  search_url = \"https://www.amazon.in/s?k=\" + product\n",
    "\n",
    "  response = requests.get(search_url)\n",
    "\n",
    "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "  product_titles = soup.find_all(\"span\", class_=\"a-size-medium a-color-base a-text-normal\")\n",
    "\n",
    "  return [product_title.text for product_title in product_titles]\n",
    "\n",
    "product = input(\"Enter the product to search: \")\n",
    "\n",
    "product_titles = search_amazon(product)\n",
    "\n",
    "print(\"Products found:\")\n",
    "for product_title in product_titles:\n",
    "  print(product_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3efcf0",
   "metadata": {},
   "source": [
    "# 2. In the above question, now scrape the following details of each product listed in first 3 pages of your search  results and save it in a data frame and csv. In case if any product has less than 3 pages in search results then  scrape all the products available under that product name. Details to be scraped are: \"Brand  Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and  “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08e56e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the product to search: car\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_amazon_product_details(product_url):\n",
    "  \n",
    "  response = requests.get(product_url)\n",
    "  \n",
    "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
    " \n",
    "  brand_name = soup.find(\"span\", class_=\"a-size-medium a-color-base a-text-normal\").text\n",
    " \n",
    "  product_name = soup.find(\"span\", class_=\"a-size-large a-color-base a-text-normal\").text\n",
    "  \n",
    "  price = soup.find(\"span\", class_=\"a-price-whole\").text\n",
    " \n",
    "  return_exchange = soup.find(\"span\", class_=\"a-size-small a-color-secondary a-text-normal\").text\n",
    " \n",
    "  expected_delivery = soup.find(\"span\", class_=\"a-size-small a-color-secondary a-text-normal\").text\n",
    "  \n",
    "  availability = soup.find(\"span\", class_=\"a-size-small a-color-secondary a-text-normal\").text\n",
    "  \n",
    "  product_details = {\n",
    "    \"Brand Name\": brand_name,\n",
    "    \"Name of the Product\": product_name,\n",
    "    \"Price\": price,\n",
    "    \"Return/Exchange\": return_exchange,\n",
    "    \"Expected Delivery\": expected_delivery,\n",
    "    \"Availability\": availability,\n",
    "    \"Product URL\": product_url\n",
    "  }\n",
    "\n",
    "  return product_details\n",
    "\n",
    "def scrape_amazon_products(product, num_pages):\n",
    " \n",
    "  product_details = []\n",
    " \n",
    "  search_url = \"https://www.amazon.in/s?k=\" + product\n",
    "  \n",
    "  for page_num in range(1, num_pages + 1):\n",
    "   \n",
    "    search_url += \"&page=\" + str(page_num)\n",
    "    \n",
    "    response = requests.get(search_url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "   \n",
    "    product_links = soup.find_all(\"a\", class_=\"a-link-normal a-text-normal\")\n",
    "   \n",
    "    for product_link in product_links:\n",
    "    \n",
    "      product_url = product_link[\"href\"]\n",
    "      \n",
    "      product_details.append(scrape_amazon_product_details(product_url))\n",
    "\n",
    "  return product_details\n",
    "\n",
    "product = input(\"Enter the product to search: \")\n",
    "\n",
    "product_details = scrape_amazon_products(product, 3)\n",
    "\n",
    "df = pd.DataFrame(product_details)\n",
    "\n",
    "df.to_csv(\"amazon_products.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b84b38c",
   "metadata": {},
   "source": [
    "# 3. Write a python program to access the search bar and search button on images.google.com and scrape 10  images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e941425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_google_images(keyword):\n",
    "  \n",
    "  search_url = \"https://images.google.com/search?q=\" + keyword\n",
    " \n",
    "  response = requests.get(search_url)\n",
    " \n",
    "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
    " \n",
    "  image_thumbnails = soup.find_all(\"img\", class_=\"rg_i Q4LuWd\")\n",
    " \n",
    "  image_urls = []\n",
    "  \n",
    "  for image_thumbnail in image_thumbnails:\n",
    "    image_url = image_thumbnail[\"src\"]\n",
    "    image_urls.append(image_url)\n",
    " \n",
    "  return image_urls\n",
    "\n",
    "\n",
    "fruits_image_urls = scrape_google_images(\"fruits\")\n",
    "cars_image_urls = scrape_google_images(\"cars\")\n",
    "machine_learning_image_urls = scrape_google_images(\"Machine Learning\")\n",
    "guitar_image_urls = scrape_google_images(\"Guitar\")\n",
    "cakes_image_urls = scrape_google_images(\"Cakes\")\n",
    "\n",
    "for image_url in fruits_image_urls[:10]:\n",
    "  image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "  image.save(\"fruits/\" + image_url.split(\"/\")[-1])\n",
    "\n",
    "for image_url in cars_image_urls[:10]:\n",
    "  image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "  image.save(\"cars/\" + image_url.split(\"/\")[-1])\n",
    "\n",
    "for image_url in machine_learning_image_urls[:10]:\n",
    "  image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "  image.save(\"machine_learning/\" + image_url.split(\"/\")[-1])\n",
    "\n",
    "for image_url in guitar_image_urls[:10]:\n",
    "  image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "  image.save(\"guitar/\" + image_url.split(\"/\")[-1])\n",
    "\n",
    "for image_url in cakes_image_urls[:10]:\n",
    "  image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "  image.save(\"cakes/\" + image_url.split(\"/\")[-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c186ce27",
   "metadata": {},
   "source": [
    "# 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,  “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d10b04c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the smartphone to search for on Flipkart: oneplus\n",
      "Scraped 16 smartphone details and saved to 'flipkart_smartphones.csv'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape smartphone details from a Flipkart search page\n",
    "def scrape_flipkart_smartphones(search_query):\n",
    "    url = f\"https://www.flipkart.com/search?q={search_query.replace(' ', '+')}\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        product_details = []\n",
    "        \n",
    "        # Find all smartphone products on the page\n",
    "        products = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "        \n",
    "        for product in products:\n",
    "            try:\n",
    "                brand_name = product.find('div', {'class': '_4rR01T'}).text\n",
    "            except AttributeError:\n",
    "                brand_name = '-'\n",
    "            \n",
    "            try:\n",
    "                smartphone_name = product.find('a', {'class': 'IRpwTa'}).text\n",
    "            except AttributeError:\n",
    "                smartphone_name = '-'\n",
    "            \n",
    "            try:\n",
    "                color = product.find('a', {'class': '_2rpwqI'}).text\n",
    "            except AttributeError:\n",
    "                color = '-'\n",
    "            \n",
    "            details = product.find_all('li', {'class': 'rgWa7D'})\n",
    "            \n",
    "            ram = '-'\n",
    "            storage_rom = '-'\n",
    "            primary_camera = '-'\n",
    "            secondary_camera = '-'\n",
    "            display_size = '-'\n",
    "            battery_capacity = '-'\n",
    "            price = '-'\n",
    "            product_url = '-'\n",
    "            \n",
    "            for detail in details:\n",
    "                text = detail.text\n",
    "                if 'RAM' in text:\n",
    "                    ram = text\n",
    "                elif 'ROM' in text:\n",
    "                    storage_rom = text\n",
    "                elif 'Primary Camera' in text:\n",
    "                    primary_camera = text\n",
    "                elif 'Secondary Camera' in text:\n",
    "                    secondary_camera = text\n",
    "                elif 'Display Size' in text:\n",
    "                    display_size = text\n",
    "                elif 'Battery Capacity' in text:\n",
    "                    battery_capacity = text\n",
    "                elif '₹' in text:\n",
    "                    price = text\n",
    "                    product_url = product.find('a', {'class': 'IRpwTa'})['href']\n",
    "            \n",
    "            product_details.append({\n",
    "                'Brand Name': brand_name,\n",
    "                'Smartphone Name': smartphone_name,\n",
    "                'Colour': color,\n",
    "                'RAM': ram,\n",
    "                'Storage(ROM)': storage_rom,\n",
    "                'Primary Camera': primary_camera,\n",
    "                'Secondary Camera': secondary_camera,\n",
    "                'Display Size': display_size,\n",
    "                'Battery Capacity': battery_capacity,\n",
    "                'Price': price,\n",
    "                'Product URL': f\"https://www.flipkart.com{product_url}\"\n",
    "            })\n",
    "        \n",
    "        return product_details\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Search query (e.g., \"Oneplus Nord\", \"pixel 4A\", etc.)\n",
    "search_query = input(\"Enter the smartphone to search for on Flipkart: \")\n",
    "\n",
    "# Scrape smartphone details from the first page of search results\n",
    "smartphone_details = scrape_flipkart_smartphones(search_query)\n",
    "\n",
    "if smartphone_details:\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(smartphone_details)\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('flipkart_smartphones.csv', index=False)\n",
    "    \n",
    "    print(f\"Scraped {len(df)} smartphone details and saved to 'flipkart_smartphones.csv'.\")\n",
    "else:\n",
    "    print(\"Failed to retrieve smartphone details.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c662e49a",
   "metadata": {},
   "source": [
    "# 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "466297ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_geospatial_coordinates(city):\n",
    "    # Define the URL of the website you want to scrape.\n",
    "    url = f\"https://example.com/{city}\"\n",
    "\n",
    "    # Send an HTTP GET request to the URL.\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Check if the request was successful (status code 200).\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content of the page using BeautifulSoup.\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Find the element containing the geospatial coordinates.\n",
    "        coordinates_element = soup.find(\"div\", class_=\"widget-subheading\")\n",
    "\n",
    "        # Check if the element was found.\n",
    "        if coordinates_element:\n",
    "            # Extract the geospatial coordinates from the element.\n",
    "            coordinates = coordinates_element.text.split(\",\")\n",
    "\n",
    "            # Check if there are at least two values (latitude and longitude).\n",
    "            if len(coordinates) >= 2:\n",
    "                # Return the latitude and longitude as floats.\n",
    "                return float(coordinates[0]), float(coordinates[1])\n",
    "\n",
    "    # If any part of the process fails, return None.\n",
    "    return None\n",
    "\n",
    "# Get the city input from the user.\n",
    "city = input(\"Enter the city to search for: \")\n",
    "\n",
    "# Scrape the geospatial coordinates of the city.\n",
    "result = scrape_geospatial_coordinates(city)\n",
    "\n",
    "# Check if the result is not None before using it.\n",
    "if result:\n",
    "    latitude, longitude = result\n",
    "    # Print the geospatial coordinates.\n",
    "    print(\"Latitude:\", latitude)\n",
    "    print(\"Longitude:\", longitude)\n",
    "else:\n",
    "    print(\"Coordinates not found for the specified city.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f61e05",
   "metadata": {},
   "source": [
    "# 6. Write a program to scrap all the available details of best gaming laptops from digit.in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5984b31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "\n",
    "def scrape_digit_in_gaming_laptops():\n",
    "  \n",
    "  # Construct the Digit.in best gaming laptops URL.\n",
    "  url = \"https://www.digit.in/laptops/best-gaming-laptops\"\n",
    "\n",
    "  # Make a GET request to the URL.\n",
    "  response = requests.get(url)\n",
    "\n",
    "  # Parse the HTML response using Beautiful Soup.\n",
    "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "  # Find all laptop table rows.\n",
    "  laptop_table_rows = soup.find_all(\"tr\", class_=\"laptop-row\")\n",
    "\n",
    "  # Create a list of dictionaries to store the laptop details.\n",
    "  laptop_details = []\n",
    "\n",
    "  # Iterate over the laptop table rows and scrape the laptop details.\n",
    "  for laptop_table_row in laptop_table_rows:\n",
    "\n",
    "    # Get the laptop name.\n",
    "    laptop_name = laptop_table_row.find(\"td\", class_=\"laptop-name\").text\n",
    "\n",
    "    # Get the laptop image URL.\n",
    "    laptop_image_url = laptop_table_row.find(\"img\")[\"src\"]\n",
    "\n",
    "    # Get the laptop specifications.\n",
    "    laptop_specifications = laptop_table_row.find(\"td\", class_=\"laptop-specs\").text\n",
    "\n",
    "    # Create a dictionary to store the laptop details.\n",
    "    laptop_details_dict = {\n",
    "      \"Laptop Name\": laptop_name,\n",
    "      \"Laptop Image\": laptop_image_url,\n",
    "      \"Laptop Specifications\": laptop_specifications\n",
    "    }\n",
    "\n",
    "    # Add the laptop details dictionary to the list of laptop details.\n",
    "    laptop_details.append(laptop_details_dict)\n",
    "\n",
    "  return laptop_details\n",
    "\n",
    "# Scrape the best gaming laptops from Digit.in.\n",
    "laptop_details = scrape_digit_in_gaming_laptops()\n",
    "\n",
    "# Download the laptop images.\n",
    "for laptop_details_dict in laptop_details:\n",
    "\n",
    "  # Get the laptop image URL.\n",
    "  laptop_image_url = laptop_details_dict[\"Laptop Image\"]\n",
    "\n",
    "  # Download the laptop image.\n",
    "  image = Image.open(requests.get(laptop_image_url, stream=True).raw)\n",
    "\n",
    "  # Save the laptop image.\n",
    "  image.save(\"laptop_images/\" + laptop_details_dict[\"Laptop Name\"] + \".jpg\")\n",
    "\n",
    "# Print the laptop details.\n",
    "for laptop_details_dict in laptop_details:\n",
    "  print(\"Laptop Name:\", laptop_details_dict[\"Laptop Name\"])\n",
    "  print(\"Laptop Image URL:\", laptop_details_dict[\"Laptop Image\"])\n",
    "  print(\"Laptop Specifications:\", laptop_details_dict[\"Laptop Specifications\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204fc149",
   "metadata": {},
   "source": [
    "# 7. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5958954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_forbes_billionaires():\n",
    "  # Construct the Forbes billionaires URL.\n",
    "  url = \"https://www.forbes.com/billionaires/list/\"\n",
    "\n",
    "  # Make a GET request to the URL.\n",
    "  response = requests.get(url)\n",
    "\n",
    "  # Parse the HTML response using Beautiful Soup.\n",
    "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "  # Find all billionaire table rows.\n",
    "  billionaire_table_rows = soup.find_all(\"tr\", class_=\"position\")\n",
    "\n",
    "  # Create a list of dictionaries to store the billionaire details.\n",
    "  billionaire_details = []\n",
    "\n",
    "  # Iterate over the billionaire table rows and scrape the billionaire details.\n",
    "  for billionaire_table_row in billionaire_table_rows:\n",
    "\n",
    "    # Get the billionaire rank.\n",
    "    billionaire_rank = billionaire_table_row.find(\"td\", class_=\"rank\").text\n",
    "\n",
    "    # Get the billionaire name.\n",
    "    billionaire_name = billionaire_table_row.find(\"td\", class_=\"name\").text\n",
    "\n",
    "    # Get the billionaire net worth.\n",
    "    billionaire_net_worth = billionaire_table_row.find(\"td\", class_=\"netWorth\").text\n",
    "\n",
    "    # Get the billionaire age.\n",
    "    billionaire_age = billionaire_table_row.find(\"td\", class_=\"age\").text\n",
    "\n",
    "    # Get the billionaire citizenship.\n",
    "    billionaire_citizenship = billionaire_table_row.find(\"td\", class_=\"citizenship\").text\n",
    "\n",
    "    # Get the billionaire source of wealth.\n",
    "    billionaire_source_of_wealth = billionaire_table_row.find(\"td\", class_=\"source\").text\n",
    "\n",
    "    # Get the billionaire industry.\n",
    "    billionaire_industry = billionaire_table_row.find(\"td\", class_=\"industry\").text\n",
    "\n",
    "    # Create a dictionary to store the billionaire details.\n",
    "    billionaire_details_dict = {\n",
    "      \"Rank\": billionaire_rank,\n",
    "      \"Name\": billionaire_name,\n",
    "      \"Net Worth\": billionaire_net_worth,\n",
    "      \"Age\": billionaire_age,\n",
    "      \"Citizenship\": billionaire_citizenship,\n",
    "      \"Source of Wealth\": billionaire_source_of_wealth,\n",
    "      \"Industry\": billionaire_industry\n",
    "    }\n",
    "\n",
    "    # Add the billionaire details dictionary to the list of billionaire details.\n",
    "    billionaire_details.append(billionaire_details_dict)\n",
    "\n",
    "      return billionaire_details\n",
    "\n",
    "# Scrape the details for all billionaires from Forbes.com.\n",
    "billionaire_details = scrape_forbes_billionaires()\n",
    "\n",
    "# Print the billionaire details.\n",
    "for billionaire_details_dict in billionaire_details:\n",
    "      print(\"Rank:\", billionaire_details_dict[\"Rank\"])\n",
    "      print(\"Name:\", billionaire_details_dict[\"Name\"])\n",
    "      print(\"Net Worth:\", billionaire_details_dict[\"Net Worth\"])\n",
    "      print(\"Age:\", billionaire_details_dict[\"Age\"])\n",
    "      print(\"Citizenship:\", billionaire_details_dict[\"Citizenship\"])\n",
    "      print(\"Source of Wealth:\", billionaire_details_dict[\"Source of Wealth\"])\n",
    "      print(\"Industry:\", billionaire_details_dict[\"Industry\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d80d648",
   "metadata": {},
   "source": [
    "# 8. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted  from any YouTube Video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898f223b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade google-api-python-client\n",
    "\n",
    "import os\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "from datetime import datetime\n",
    "\n",
    "# Set your YouTube API key here.\n",
    "API_KEY = 'YOUR_API_KEY'\n",
    "\n",
    "# YouTube video ID of the video you want to extract comments from.\n",
    "VIDEO_ID = 'YOUR_VIDEO_ID'\n",
    "\n",
    "# Number of comments to fetch (at least 500).\n",
    "MAX_COMMENTS = 500\n",
    "\n",
    "# Initialize the YouTube Data API client.\n",
    "youtube = googleapiclient.discovery.build('youtube', 'v3', developerKey=API_KEY)\n",
    "\n",
    "def fetch_comments(video_id, max_results):\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "\n",
    "    while len(comments) < max_results:\n",
    "        try:\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                textFormat=\"plainText\",\n",
    "                maxResults=min(100, max_results - len(comments)),\n",
    "                pageToken=next_page_token\n",
    "            )\n",
    "            response = request.execute()\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                comment = item[\"snippet\"][\"topLevelComment\"]\n",
    "                comment_text = comment[\"snippet\"][\"textDisplay\"]\n",
    "                comment_upvotes = comment[\"snippet\"][\"likeCount\"]\n",
    "                comment_time = datetime.fromisoformat(comment[\"snippet\"][\"publishedAt\"][:-1])\n",
    "\n",
    "                comments.append({\n",
    "                    \"text\": comment_text,\n",
    "                    \"upvotes\": comment_upvotes,\n",
    "                    \"time\": comment_time\n",
    "                })\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "\n",
    "            if not next_page_token:\n",
    "                break\n",
    "\n",
    "        except googleapiclient.errors.HttpError as e:\n",
    "            print(\"An error occurred:\", str(e))\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    all_comments = fetch_comments(VIDEO_ID, MAX_COMMENTS)\n",
    "\n",
    "    # Print the first 10 comments as an example.\n",
    "    for i, comment in enumerate(all_comments[:10], start=1):\n",
    "        print(f\"Comment {i}:\")\n",
    "        print(\"Text:\", comment[\"text\"])\n",
    "        print(\"Upvotes:\", comment[\"upvotes\"])\n",
    "        print(\"Time:\", comment[\"time\"].strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        print()\n",
    "\n",
    "    # You now have all the comments, upvotes, and timestamps in the 'all_comments' list.\n",
    "    # You can process, analyze, or save this data as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd60721",
   "metadata": {},
   "source": [
    "# 9. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in  “London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews, overall  reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8892913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_hostelworld_hostels(location):\n",
    " \n",
    "    \n",
    "  url = \"https://www.hostelworld.com/search?query=\" + location\n",
    "\n",
    "  # Make a GET request to the URL.\n",
    "  response = requests.get(url)\n",
    "\n",
    "  # Parse the HTML response using Beautiful Soup.\n",
    "  soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "  # Find all hostel listings.\n",
    "  hostel_listings = soup.find_all(\"div\", class_=\"listing-card\")\n",
    "\n",
    "  # Create a list of dictionaries to store the hostel details.\n",
    "  hostel_details = []\n",
    "\n",
    "  # Iterate over the hostel listings and scrape the hostel details.\n",
    "  for hostel_listing in hostel_listings:\n",
    "\n",
    "    # Get the hostel name.\n",
    "    hostel_name = hostel_listing.find(\"h2\", class_=\"listing-name\").text\n",
    "\n",
    "    # Get the distance from city centre.\n",
    "    distance_from_city_centre = hostel_listing.find(\"span\", class_=\"distance\").text\n",
    "\n",
    "    # Get the ratings.\n",
    "    ratings = hostel_listing.find(\"span\", class_=\"rating\").text\n",
    "\n",
    "    # Get the total reviews.\n",
    "    total_reviews = hostel_listing.find(\"span\", class_=\"review-count\").text\n",
    "\n",
    "    # Get the overall reviews.\n",
    "    overall_reviews = hostel_listing.find(\"span\", class_=\"overall-rating\").text\n",
    "\n",
    "    # Get the privates from price.\n",
    "    privates_from_price = hostel_listing.find(\"span\", class_=\"price private\").text\n",
    "\n",
    "    # Get the dorms from price.\n",
    "    dorms_from_price = hostel_listing.find(\"span\", class_=\"price dorm\").text\n",
    "\n",
    "    # Get the facilities.\n",
    "    facilities = hostel_listing.find(\"ul\", class_=\"facilities\").text\n",
    "\n",
    "    # Get the property description.\n",
    "    property_description = hostel_listing.find(\"p\", class_=\"description\").text\n",
    "\n",
    "  \n",
    "    hostel_details_dict = {\n",
    "      \"Hostel Name\": hostel_name,\n",
    "      \"Distance from City Centre\": distance_from_city_centre,\n",
    "      \"Ratings\": ratings,\n",
    "      \"Total Reviews\": total_reviews,\n",
    "      \"Overall Reviews\": overall_reviews,\n",
    "      \"Privates from Price\": privates_from_price,\n",
    "      \"Dorms from Price\": dorms_from_price,\n",
    "      \"Facilities\": facilities,\n",
    "      \"Property Description\": property_description\n",
    "    }\n",
    "\n",
    "    \n",
    "    hostel_details.append(hostel_details_dict)\n",
    "\n",
    "  return hostel_details\n",
    "\n",
    "hostel_details = scrape_hostelworld_hostels(\"London\")\n",
    "\n",
    "\n",
    "for hostel_details_dict in hostel_details:\n",
    "  print(\"Hostel Name:\", hostel_details_dict[\"Hostel Name\"])\n",
    "  print(\"Distance from City Centre:\", hostel_details_dict[\"Distance from City Centre\"])\n",
    "  print(\"Ratings:\", hostel_details_dict[\"Ratings\"])\n",
    "  print(\"Total Reviews:\", hostel_details_dict[\"Total Reviews\"])\n",
    "  print(\"Overall Reviews:\", hostel_details_dict[\"Overall Reviews\"])\n",
    "  print(\"Privates from Price:\", hostel_details_dict[\"Privates from Price\"])\n",
    "  print(\"Dorms from Price:\", hostel_details_dict[\"Dorms from Price\"])\n",
    "  print(\"Facilities:\", hostel_details_dict[\"Facilities\"])\n",
    "  print(\"Property Description:\", hostel_details_dict[\"Property Description\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45258cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
